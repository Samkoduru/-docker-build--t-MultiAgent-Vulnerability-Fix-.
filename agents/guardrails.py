import re

def sanitize_input(text: str) -> str:
    """
    Basic example to sanitize user input before sending to LLM.
    1. Removes any known secret patterns.
    2. Can also remove or mask emails, tokens, etc.
    """
    # Naive example: mask anything that looks like a token or key
    text = re.sub(r"sk-[0-9a-zA-Z]+", "[REDACTED_API_KEY]", text)
    # Mask GitHub tokens (ghp_...)
    text = re.sub(r"ghp_[0-9a-zA-Z]+", "[REDACTED_GITHUB_TOKEN]", text)

    return text

def detect_pii_or_secrets(output: str) -> bool:
    """
    Checks if the LLM output potentially leaks secrets or PII.
    For demonstration: we look for patterns like email, phone, or token prefixes.
    Returns True if suspicious content is found, else False.
    """
    # Check for email-like patterns
    if re.search(r"[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\.[a-zA-Z0-9-.]+", output):
        return True
    
    # Check for token patterns (sk-, ghp-, etc.)
    if "sk-" in output or "ghp_" in output:
        return True

    # You could add phone # regex or credit card # checks
    return False

def verify_output(output: str) -> str:
    """
    If suspicious content is detected, raise an exception or sanitize further.
    """
    if detect_pii_or_secrets(output):
        raise ValueError("LLM output contained possible PII or secrets! Aborting.")
    return output
